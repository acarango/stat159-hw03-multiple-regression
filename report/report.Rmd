---
title: "HW03: Multiple Regression"
author: "Austin Carango"
date: "December 9, 2016"
output: html_document
---

# Abstract

The idea of this project is to perform multiple least squares regression on the Advertising data set. This dataset reports 200 observations of `Sales`, `TV`, `Radio`, and `Newspaper`. This data is freely available ass a CSV file at [http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv). The methods discused are based on Chapter 3: Linear Regression (from " _An Introduction to Statistical Learning_" by James et al), which is available at http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf.


# Introduction

Given a data set containing information pertaining to advertising budgets for three different media (TV, Radio, and Newspaper), how can we model sales of a product?

To answer this question, this report will discuss multiple linear regression. We will attempt to model sales in thousands of dollars as a function of three variables: `TV`, `Radio` and `Newspaper`. Because we are modelling this relationship via linear regression, we are inherently assuming that a linear relationship exists between our dependent variables and independent variable. We will run this regression, and then comment on its validity.

# Data

The data set utilized in this paper, `Advertising.csv`, contains 200 observations of 5 variables. Each observation corresponds to one firm, and 4 of the variables are of interest: `Sales`, `Radio`, `TV` and `Newspaper`. The  variable `X` is an index.

This dataset is free to access online at the following url http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv.

We will use all 4 variables of interest in this analysis. `Sales` refers to the sales of a particular product in thousands of dollars. `TV` refers to the firm's TV advertising budget in thousands of dollars. `Radio` refers to the firm's radio advertising mudget in thousands of dollars. `Newspaper` refers to the firm's newspaper advertising budget in thousands of dollars.

In our analysis, we will treat `Sales` as the dependent variable and the other 3 variables of interest as the independent variables.

# Methodology 

For this linear regression, we treat `Sales` as the response variable. This means `Sales` will depend on a function of some other variables. `TV`, `Newspaper` and `Radio` are the explanatory variables, meaning their values determine the value of `Sales` via a function. We use the simple linear model below to estimate the relationship between Sales and the three explanatory variables.

$$ Sales = \beta_0 + \beta_1*TV + \beta_2*Radio + \beta_3*Newspaper $$

We have 4 beta coefficients, 3 of which determine the slope and one (beta0) which is the intercept.

In order to fit a line to this data we must minimize the squared error. In other words we must find a line that minimizes the squared error, error being the distance between actual and fitted values. We accomplish this by minimizing the expression below.

$$\sum_{i=1}^n (y_i - \beta_0 - \beta_1x_1i - \beta_2x_2i - \beta_3x_3i)^2 $$

# Results

Performing simple least squares regression seperately for each of the three explanatory variables produces the following coefficient estimates:

```{r, echo=FALSE} 
load(file="../data/regression.RData")
print(as.table(newspaper_lm$coefficients))
print(as.table(radio_lm$coefficients))
print(as.table(tv_lm$coefficients))
```

Performing multiple ordinary least squares with all three explanatory variables at once as described in the methodology section provides us with the following coefficients:

```{r, echo=FALSE}
coef_tbbl<-as.table(advertising_lm$coefficients)
print(coef_tbbl)    
```

Here is the correlation matrix:

```{r, echo=FALSE}
load("../data/correlation-matrix.RData")
print(as.table(cor_mat))
```

Using the functions created in `code/functions/regression-functions.R` we get the following values for RSS, TSS, R^2, F-statistic, and RSE:


```{r, echo = FALSE}
source("../code/functions/regression-functions.R")

vec <- c(residual_sum_squares(advertising_lm), total_sum_squares (advertising_lm), r_squared(advertising_lm), f_statistic(advertising_lm), residual_std_error(advertising_lm))
names(vec) <- c("RSS", "TSS", "R2", "F-stat", "RSE")
coef_tbbl<-as.table(vec)
print(coef_tbbl)            
```


Here are some useful diagnostic plots for this regression as well:

![Residual Plot](../images/residual-plot.png)

![Scale-Location Plot](../images/scale-location-plot.png)

![Normal QQ Plot](../images/normal-qq-plot.png)

# Conclusions

There are several conclusions to be made based off the above results:

* Firstly, we see that the most positive coefficient in multiple least squares is for `Radio` and the most negative coefficient is for `Newspaper`, which is mildly negative. `TV` has a positive effect but it is much smaller than `Radio`. When we use `Newspaper` as the only variable, we get a positive correlation however. Looking at the correlation matrix, it is evident that `Radio` and `Newspaper` have a fairly significant correlation with one another, moreso than either has with TV. This could explain why the effect of `Newspaper` is being muted by multiple least squares, and it may be useful to drop either `Newspaper` or `Radio` from the model in order to mitigate problems associated with collinearity of these variables. 

* Second, we see that the R squared value of this regression is relatively high. R squared takes values between 0 and 1. A value of 0 means the model explains none of the variation in the data, whereas a value of 1 means the model explains all of said variation. The value we get is nearly 0.9, which means most of the variation in the data is explained by the model.

* Thirdly, looking at the diagnostic plots we can see some issues with the model. The residuals vs fitted plot as well as the scale-location plot have a u shape; this indicates that there may be some non-linear relationship at play in the data which could be better captured with another model. There appears to be heteroscedasticity in the distribution of the residuals as well, which violates one of the key assumptions in the Gauss-Markov theorem, which states that OLS provides the best linear unbiased coefficient estimtes; this is further evidence that a better model for the data is possible From the normal-qq plot we can see that the residuals have a right-skewed distribution. This could make prediction intervals inaccurate, put for the purposes of this report, non-normal residuals does not pose issues due to the sample size.

In conclusion this model has its disadvantages, but is fairly accurate. `TV` at least seems to be a good predictor as it is correlated with `Sales` but not very correlated with any of the other predictors. The model fits the data very well, but we could drop either `Radio` or `Newspaper` and most likely get more reliable results by removing collinearity. Predictive accuracy could be improved by a model that accounts for the non-linearity we see in the residual vs fitted plot as well as evident heteroscedasticity in errors.
